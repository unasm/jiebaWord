{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MySQLdb as mysql\n",
    "import math\n",
    "import os\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import zipfile\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# 有待改进， 可以首先通 pr 算法生成相应的 关键词，然后再优化词向量的筛选过程，getWords 使词向量都是 同一个文章内的的关键字，然后去看效果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n",
      "('count is ', 20033)\n",
      "[['UNK', 17005207], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 20033;\n",
    "csvPath = \"/Users/tianyi/project/dl-workshop-rnn-lstm/track_key.csv\"\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "        'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "  \n",
    "def testGetWords():\n",
    "    filename = maybe_download('text8.zip', 31344016)\n",
    "    words = read_data(filename)\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "        unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "dataEncode, count, dictionary, reverse_dictionary = testGetWords() \n",
    "print(\"count is \" , len(count))\n",
    "print(count[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "((9120030L,),)\n"
     ]
    }
   ],
   "source": [
    "conn = mysql.connect('localhost', 'root', '', 'analyze')\n",
    "def GetList(sql):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql)\n",
    "    data = cur.fetchall()\n",
    "    return data\n",
    "dlist = GetList(\"select count(*) from track_key\")\n",
    "print(len(dlist))\n",
    "print(dlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " select track_key.word, track_key.count, track_key.article_id, word_attr.flag  from track_key inner join word_attr on \n",
      "            word_attr.word = track_key.word where word_attr.flag in \n",
      "            (select distinct flag as flag  from word_attr where flag like '%n%')  and track_key.is_delete = 0\n",
      "    \n",
      "count :  4569576\n",
      "save to /Users/tianyi/project/dl-workshop-rnn-lstm/track_key.csv\n"
     ]
    }
   ],
   "source": [
    "def putWordsText():\n",
    "     # 只要名次\n",
    "    sql = \"\"\" select track_key.word, track_key.count, track_key.article_id, word_attr.flag  from track_key inner join word_attr on \n",
    "            word_attr.word = track_key.word where word_attr.flag in \n",
    "            (select distinct flag as flag  from word_attr where flag like '%n%')  and track_key.is_delete = 0\n",
    "    \"\"\"\n",
    "    print(sql)\n",
    "    keys = GetList(sql)\n",
    "    fp = open(csvPath, 'a')\n",
    "    header = [\"word\", \"count\", \"article_id\", \"flag\"]\n",
    "    csv_writer = csv.writer(fp)\n",
    "    csv_writer.writerow(header)\n",
    "    for line in keys:\n",
    "        csv_writer.writerow(line)\n",
    "    fp.close()\n",
    "    print \"count : \", len(keys)\n",
    "    print \"save to %s\" % csvPath\n",
    "putWordsText() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def getWordsText():\n",
    "    with open(csvPath, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        column = []\n",
    "        for row in reader:\n",
    "            column.extend([row])\n",
    "            #print row[\"count\"]\n",
    "        #print column[0][\"count\"\n",
    "    return \n",
    "getWordsText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['UNK', 0], ('A\\xe8\\x82\\xa1', 84), ('AT&T', 14), ('T\\xe6\\x81\\xa4', 1), ('C++', 1)]\n"
     ]
    }
   ],
   "source": [
    "def getWords(): \n",
    "    # 只要名次\n",
    "    sql = \"\"\" select track_key.*, word_attr.flag  from track_key inner join word_attr on \n",
    "            word_attr.word = track_key.word where word_attr.flag in \n",
    "            (select distinct flag as flag  from word_attr where flag like '%n%')\n",
    "    \"\"\"\n",
    "    #print(sql)\n",
    "    #keys = GetList(sql)\n",
    "    words = {}\n",
    "    keys = []\n",
    "    with open(csvPath, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            keys.extend([row])\n",
    "    for key in keys:\n",
    "        word = key[\"word\"]\n",
    "        artId = key[\"article_id\"]\n",
    "        cnt = key[\"count\"]\n",
    "        if not words.has_key(artId):\n",
    "            words[artId] = {}\n",
    "        words[artId][word] = cnt \n",
    "\n",
    "    allWords = []\n",
    "    for art in words:\n",
    "        allWords.extend(words[art].keys())\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(allWords).most_common(vocabulary_size))\n",
    "\n",
    "    cnt = 0\n",
    "    dictionary = {}\n",
    "    for word,_ in count:\n",
    "        dictionary[word] = cnt\n",
    "        cnt = cnt + 1\n",
    "    dataEncode = list()\n",
    "    unk_count = 0\n",
    "    for word in allWords:\n",
    "        index = 0\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            unk_count = unk_count + 1\n",
    "        dataEncode.append(index)\n",
    "    count[0][1] = unk_count\n",
    "\n",
    "    reverse_dictinary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dataEncode, count, dictionary, reverse_dictinary\n",
    "_, count, _, _ = getWords() \n",
    "\n",
    "print(count[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "with num_skips = 2 and skip_window = 1:\n",
      "3084 _______________\n",
      "3084 _______________\n",
      "3084 _______________\n",
      "3084 _______________\n",
      "3084 _______________\n",
      "3084 _______________\n",
      "3084 _______________\n",
      "3084 _______________\n",
      "[5239] [12] [5239] [12] [12] [5239] [5239] [12] \n",
      "with num_skips = 4 and skip_window = 2:\n",
      "12 _______________\n",
      "12 _______________\n",
      "12 _______________\n",
      "12 _______________\n",
      "12 _______________\n",
      "12 _______________\n",
      "12 _______________\n",
      "12 _______________\n",
      "[5239] [6] [195] [3084] [5239] [3084] [195] [6]\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "def generate_batch(batch_size, num_skips, skip_window, data):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [ skip_window ]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]        \n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels\n",
    "#dataEncode, count, dictionary, reverse_dictionary = testGetWords() \n",
    "for num_skips, skip_window in [(2, 1), (4, 2)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window, data=dataEncode)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "    for val in batch:\n",
    "        print val, \n",
    "        print \"_______________\"\n",
    "    for val in labels:\n",
    "        print val, \n",
    "    #print \" \".join(batch)\n",
    "    #print \" \".join(labels)\n",
    "\n",
    "    #print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    #print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__main__\n",
      " select track_key.*, word_attr.flag  from track_key inner join word_attr on \n",
      "            word_attr.word = track_key.word where word_attr.flag in \n",
      "            (select distinct flag as flag  from word_attr where flag like '%n%')\n",
      "    \n",
      "4627469\n"
     ]
    }
   ],
   "source": [
    "print __name__\n",
    "embedding_size = 128\n",
    "stepsNum = 10001;\n",
    "valid_window = 100\n",
    "valid_size = 16\n",
    "batch_size = 256\n",
    "# 随机选择用来测试 运算效果的case\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "if __name__ == \"__main__\": \n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default(), tf.device('/cpu:0'):\n",
    "        train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "        train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "        embedding = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "        #biases = tf.zeros(batch_size) + 0.01\n",
    "        biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "        softmax_weight = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "        lookedEmbdding = tf.nn.embedding_lookup(embedding, train_dataset)\n",
    "        num_sampled = 64\n",
    "        loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(\n",
    "            weights=softmax_weight, \n",
    "            biases=biases,\n",
    "            inputs=lookedEmbdding, \n",
    "            num_sampled=num_sampled,\n",
    "            labels=train_labels, \n",
    "            num_classes=vocabulary_size)\n",
    "        )\n",
    "        optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "\n",
    "        norm = tf.reduce_sum(tf.square(embedding), 1 , keep_dims=True)\n",
    "        embd_norm = embedding / tf.sqrt(norm)\n",
    "        valid_embeddings = tf.nn.embedding_lookup(embd_norm, valid_dataset)\n",
    "        similarity = tf.matmul(embd_norm, tf.transpose(valid_embeddings))\n",
    "    #dataEncode, count, dictionary, reverse_dictionary = testGetWords() \n",
    "    dataEncode, count, dictionary, reverse_dictionary = getWords() \n",
    "\n",
    "    print(len(dataEncode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "10001\n",
      "0.0066512260437\n",
      "now get data %s 0\n",
      "Nearest to UNK : ,  投资\t,  行业\t,  市场\t,  重点\t,  板块\t,  公司\t,  中国\t,  风险\t\n",
      "Nearest to 预期 : ,  公司\t,  发展\t,  方面\t,  建议\t,  企业\t,  投资\t,  板块\t,  市场\t\n",
      "Nearest to 规模 : ,  UNK\t,  重点\t,  中国\t,  板块\t,  企业\t,  风险\t,  市场\t,  方面\t\n",
      "Nearest to 建设 : ,  风险\t,  行业\t,  方面\t,  投资\t,  建议\t,  UNK\t,  发展\t,  政策\t\n",
      "Nearest to 投资 : ,  方面\t,  风险\t,  预期\t,  股份\t,  市场\t,  建议\t,  行业\t,  发展\t\n",
      "Nearest to 影响 : ,  政策\t,  中国\t,  预期\t,  市场\t,  UNK\t,  板块\t,  风险\t,  股份\t\n",
      "Nearest to 数据 : ,  中国\t,  公司\t,  建议\t,  发展\t,  UNK\t,  市场\t,  风险\t,  政策\t\n",
      "Nearest to 竞争 : ,  方面\t,  板块\t,  企业\t,  风险\t,  行业\t,  市场\t,  中国\t,  重点\t\n",
      "Nearest to 估值 : ,  中国\t,  建议\t,  方面\t,  企业\t,  公司\t,  投资\t,  发展\t,  风险\t\n",
      "Nearest to 环比 : ,  企业\t,  发展\t,  公司\t,  行业\t,  方面\t,  股份\t,  板块\t,  建议\t\n",
      "Nearest to 国际 : ,  板块\t,  企业\t,  政策\t,  中国\t,  UNK\t,  发展\t,  投资\t,  公司\t\n",
      "Nearest to 国家 : ,  发展\t,  预期\t,  UNK\t,  企业\t,  重点\t,  股份\t,  风险\t,  公司\t\n",
      "Nearest to 公司 : ,  建议\t,  预期\t,  方面\t,  投资\t,  行业\t,  发展\t,  公司\t,  板块\t\n",
      "Nearest to 销售 : ,  市场\t,  重点\t,  预期\t,  建议\t,  板块\t,  政策\t,  风险\t,  公司\t\n",
      "Nearest to 布局 : ,  预期\t,  市场\t,  建议\t,  企业\t,  风险\t,  重点\t,  中国\t,  板块\t\n",
      "Nearest to 行情 : ,  板块\t,  UNK\t,  预期\t,  市场\t,  政策\t,  发展\t,  行业\t,  风险\t\n",
      "0.0995592364906\n",
      "now get data %s 1000\n",
      "Nearest to UNK : ,  投资\t,  行业\t,  市场\t,  重点\t,  板块\t,  公司\t,  中国\t,  风险\t\n",
      "Nearest to 预期 : ,  公司\t,  发展\t,  方面\t,  建议\t,  企业\t,  投资\t,  板块\t,  市场\t\n",
      "Nearest to 规模 : ,  UNK\t,  重点\t,  中国\t,  板块\t,  企业\t,  风险\t,  市场\t,  方面\t\n",
      "Nearest to 建设 : ,  风险\t,  行业\t,  方面\t,  投资\t,  建议\t,  UNK\t,  发展\t,  政策\t\n",
      "Nearest to 投资 : ,  方面\t,  风险\t,  预期\t,  股份\t,  市场\t,  建议\t,  行业\t,  发展\t\n",
      "Nearest to 影响 : ,  政策\t,  中国\t,  预期\t,  市场\t,  UNK\t,  板块\t,  风险\t,  股份\t\n",
      "Nearest to 数据 : ,  中国\t,  公司\t,  建议\t,  发展\t,  UNK\t,  市场\t,  风险\t,  政策\t\n",
      "Nearest to 竞争 : ,  方面\t,  板块\t,  企业\t,  风险\t,  行业\t,  市场\t,  中国\t,  重点\t\n",
      "Nearest to 估值 : ,  中国\t,  建议\t,  方面\t,  企业\t,  公司\t,  投资\t,  发展\t,  风险\t\n",
      "Nearest to 环比 : ,  企业\t,  发展\t,  公司\t,  行业\t,  方面\t,  股份\t,  板块\t,  建议\t\n",
      "Nearest to 国际 : ,  板块\t,  企业\t,  政策\t,  中国\t,  UNK\t,  发展\t,  投资\t,  公司\t\n",
      "Nearest to 国家 : ,  发展\t,  预期\t,  UNK\t,  企业\t,  重点\t,  股份\t,  风险\t,  公司\t\n",
      "Nearest to 公司 : ,  建议\t,  预期\t,  方面\t,  投资\t,  行业\t,  发展\t,  公司\t,  板块\t\n",
      "Nearest to 销售 : ,  市场\t,  重点\t,  预期\t,  建议\t,  板块\t,  政策\t,  风险\t,  公司\t\n",
      "Nearest to 布局 : ,  预期\t,  市场\t,  建议\t,  企业\t,  风险\t,  重点\t,  中国\t,  板块\t\n",
      "Nearest to 行情 : ,  板块\t,  UNK\t,  预期\t,  市场\t,  政策\t,  发展\t,  行业\t,  风险\t\n",
      "0.00945856779589\n",
      "now get data %s 2000\n",
      "Nearest to UNK : ,  投资\t,  行业\t,  市场\t,  重点\t,  板块\t,  公司\t,  中国\t,  风险\t\n",
      "Nearest to 预期 : ,  公司\t,  发展\t,  方面\t,  建议\t,  企业\t,  投资\t,  板块\t,  市场\t\n",
      "Nearest to 规模 : ,  UNK\t,  重点\t,  中国\t,  板块\t,  企业\t,  风险\t,  市场\t,  方面\t\n",
      "Nearest to 建设 : ,  风险\t,  行业\t,  方面\t,  投资\t,  建议\t,  UNK\t,  发展\t,  政策\t\n",
      "Nearest to 投资 : ,  方面\t,  风险\t,  预期\t,  股份\t,  市场\t,  建议\t,  行业\t,  发展\t\n",
      "Nearest to 影响 : ,  政策\t,  中国\t,  预期\t,  市场\t,  UNK\t,  板块\t,  风险\t,  股份\t\n",
      "Nearest to 数据 : ,  中国\t,  公司\t,  建议\t,  发展\t,  UNK\t,  市场\t,  风险\t,  政策\t\n",
      "Nearest to 竞争 : ,  方面\t,  板块\t,  企业\t,  风险\t,  行业\t,  市场\t,  中国\t,  重点\t\n",
      "Nearest to 估值 : ,  中国\t,  建议\t,  方面\t,  企业\t,  公司\t,  投资\t,  发展\t,  风险\t\n",
      "Nearest to 环比 : ,  企业\t,  发展\t,  公司\t,  行业\t,  方面\t,  股份\t,  板块\t,  建议\t\n",
      "Nearest to 国际 : ,  板块\t,  企业\t,  政策\t,  中国\t,  UNK\t,  发展\t,  投资\t,  公司\t\n",
      "Nearest to 国家 : ,  发展\t,  预期\t,  UNK\t,  企业\t,  重点\t,  股份\t,  风险\t,  公司\t\n",
      "Nearest to 公司 : ,  建议\t,  预期\t,  方面\t,  投资\t,  行业\t,  发展\t,  公司\t,  板块\t\n",
      "Nearest to 销售 : ,  市场\t,  重点\t,  预期\t,  建议\t,  板块\t,  政策\t,  风险\t,  公司\t\n",
      "Nearest to 布局 : ,  预期\t,  市场\t,  建议\t,  企业\t,  风险\t,  重点\t,  中国\t,  板块\t\n",
      "Nearest to 行情 : ,  板块\t,  UNK\t,  预期\t,  市场\t,  政策\t,  发展\t,  行业\t,  风险\t\n",
      "0.0217292089665\n",
      "now get data %s 3000\n",
      "Nearest to UNK : ,  投资\t,  行业\t,  市场\t,  重点\t,  板块\t,  公司\t,  中国\t,  风险\t\n",
      "Nearest to 预期 : ,  公司\t,  发展\t,  方面\t,  建议\t,  企业\t,  投资\t,  板块\t,  市场\t\n",
      "Nearest to 规模 : ,  UNK\t,  重点\t,  中国\t,  板块\t,  企业\t,  风险\t,  市场\t,  方面\t\n",
      "Nearest to 建设 : ,  风险\t,  行业\t,  方面\t,  投资\t,  建议\t,  UNK\t,  发展\t,  政策\t\n",
      "Nearest to 投资 : ,  方面\t,  风险\t,  预期\t,  股份\t,  市场\t,  建议\t,  行业\t,  发展\t\n",
      "Nearest to 影响 : ,  政策\t,  中国\t,  预期\t,  市场\t,  UNK\t,  板块\t,  风险\t,  股份\t\n",
      "Nearest to 数据 : ,  中国\t,  公司\t,  建议\t,  发展\t,  UNK\t,  市场\t,  风险\t,  政策\t\n",
      "Nearest to 竞争 : ,  方面\t,  板块\t,  企业\t,  风险\t,  行业\t,  市场\t,  中国\t,  重点\t\n",
      "Nearest to 估值 : ,  中国\t,  建议\t,  方面\t,  企业\t,  公司\t,  投资\t,  发展\t,  风险\t\n",
      "Nearest to 环比 : ,  企业\t,  发展\t,  公司\t,  行业\t,  方面\t,  股份\t,  板块\t,  建议\t\n",
      "Nearest to 国际 : ,  板块\t,  企业\t,  政策\t,  中国\t,  UNK\t,  发展\t,  投资\t,  公司\t\n",
      "Nearest to 国家 : ,  发展\t,  预期\t,  UNK\t,  企业\t,  重点\t,  股份\t,  风险\t,  公司\t\n",
      "Nearest to 公司 : ,  建议\t,  预期\t,  方面\t,  投资\t,  行业\t,  发展\t,  公司\t,  板块\t\n",
      "Nearest to 销售 : ,  市场\t,  重点\t,  预期\t,  建议\t,  板块\t,  政策\t,  风险\t,  公司\t\n",
      "Nearest to 布局 : ,  预期\t,  市场\t,  建议\t,  企业\t,  风险\t,  重点\t,  中国\t,  板块\t\n",
      "Nearest to 行情 : ,  板块\t,  UNK\t,  预期\t,  市场\t,  政策\t,  发展\t,  行业\t,  风险\t\n",
      "0.0198832545747\n",
      "now get data %s 4000\n",
      "Nearest to UNK : ,  投资\t,  行业\t,  市场\t,  重点\t,  板块\t,  公司\t,  中国\t,  风险\t\n",
      "Nearest to 预期 : ,  公司\t,  发展\t,  方面\t,  建议\t,  企业\t,  投资\t,  板块\t,  市场\t\n",
      "Nearest to 规模 : ,  UNK\t,  重点\t,  中国\t,  板块\t,  企业\t,  风险\t,  市场\t,  方面\t\n",
      "Nearest to 建设 : ,  风险\t,  行业\t,  方面\t,  投资\t,  建议\t,  UNK\t,  发展\t,  政策\t\n",
      "Nearest to 投资 : ,  方面\t,  风险\t,  预期\t,  股份\t,  市场\t,  建议\t,  行业\t,  发展\t\n",
      "Nearest to 影响 : ,  政策\t,  中国\t,  预期\t,  市场\t,  UNK\t,  板块\t,  风险\t,  股份\t\n",
      "Nearest to 数据 : ,  中国\t,  公司\t,  建议\t,  发展\t,  UNK\t,  市场\t,  风险\t,  政策\t\n",
      "Nearest to 竞争 : ,  方面\t,  板块\t,  企业\t,  风险\t,  行业\t,  市场\t,  中国\t,  重点\t\n",
      "Nearest to 估值 : ,  中国\t,  建议\t,  方面\t,  企业\t,  公司\t,  投资\t,  发展\t,  风险\t\n",
      "Nearest to 环比 : ,  企业\t,  发展\t,  公司\t,  行业\t,  方面\t,  股份\t,  板块\t,  建议\t\n",
      "Nearest to 国际 : ,  板块\t,  企业\t,  政策\t,  中国\t,  UNK\t,  发展\t,  投资\t,  公司\t\n",
      "Nearest to 国家 : ,  发展\t,  预期\t,  UNK\t,  企业\t,  重点\t,  股份\t,  风险\t,  公司\t\n",
      "Nearest to 公司 : ,  建议\t,  预期\t,  方面\t,  投资\t,  行业\t,  发展\t,  公司\t,  板块\t\n",
      "Nearest to 销售 : ,  市场\t,  重点\t,  预期\t,  建议\t,  板块\t,  政策\t,  风险\t,  公司\t\n",
      "Nearest to 布局 : ,  预期\t,  市场\t,  建议\t,  企业\t,  风险\t,  重点\t,  中国\t,  板块\t\n",
      "Nearest to 行情 : ,  板块\t,  UNK\t,  预期\t,  市场\t,  政策\t,  发展\t,  行业\t,  风险\t\n",
      "0.0121388477678\n",
      "now get data %s 5000\n",
      "Nearest to UNK : ,  投资\t,  行业\t,  市场\t,  重点\t,  板块\t,  公司\t,  中国\t,  风险\t\n",
      "Nearest to 预期 : ,  公司\t,  发展\t,  方面\t,  建议\t,  企业\t,  投资\t,  板块\t,  市场\t\n",
      "Nearest to 规模 : ,  UNK\t,  重点\t,  中国\t,  板块\t,  企业\t,  风险\t,  市场\t,  方面\t\n",
      "Nearest to 建设 : ,  风险\t,  行业\t,  方面\t,  投资\t,  建议\t,  UNK\t,  发展\t,  政策\t\n",
      "Nearest to 投资 : ,  方面\t,  风险\t,  预期\t,  股份\t,  市场\t,  建议\t,  行业\t,  发展\t\n",
      "Nearest to 影响 : ,  政策\t,  中国\t,  预期\t,  市场\t,  UNK\t,  板块\t,  风险\t,  股份\t\n",
      "Nearest to 数据 : ,  中国\t,  公司\t,  建议\t,  发展\t,  UNK\t,  市场\t,  风险\t,  政策\t\n",
      "Nearest to 竞争 : ,  方面\t,  板块\t,  企业\t,  风险\t,  行业\t,  市场\t,  中国\t,  重点\t\n",
      "Nearest to 估值 : ,  中国\t,  建议\t,  方面\t,  企业\t,  公司\t,  投资\t,  发展\t,  风险\t\n",
      "Nearest to 环比 : ,  企业\t,  发展\t,  公司\t,  行业\t,  方面\t,  股份\t,  板块\t,  建议\t\n",
      "Nearest to 国际 : ,  板块\t,  企业\t,  政策\t,  中国\t,  UNK\t,  发展\t,  投资\t,  公司\t\n",
      "Nearest to 国家 : ,  发展\t,  预期\t,  UNK\t,  企业\t,  重点\t,  股份\t,  风险\t,  公司\t\n",
      "Nearest to 公司 : ,  建议\t,  预期\t,  方面\t,  投资\t,  行业\t,  发展\t,  公司\t,  板块\t\n",
      "Nearest to 销售 : ,  市场\t,  重点\t,  预期\t,  建议\t,  板块\t,  政策\t,  风险\t,  公司\t\n",
      "Nearest to 布局 : ,  预期\t,  市场\t,  建议\t,  企业\t,  风险\t,  重点\t,  中国\t,  板块\t\n",
      "Nearest to 行情 : ,  板块\t,  UNK\t,  预期\t,  市场\t,  政策\t,  发展\t,  行业\t,  风险\t\n",
      "0.0123430200216\n",
      "now get data %s 6000\n",
      "Nearest to UNK : ,  投资\t,  行业\t,  市场\t,  重点\t,  板块\t,  公司\t,  中国\t,  风险\t\n",
      "Nearest to 预期 : ,  公司\t,  发展\t,  方面\t,  建议\t,  企业\t,  投资\t,  板块\t,  市场\t\n",
      "Nearest to 规模 : ,  UNK\t,  重点\t,  中国\t,  板块\t,  企业\t,  风险\t,  市场\t,  方面\t\n",
      "Nearest to 建设 : ,  风险\t,  行业\t,  方面\t,  投资\t,  建议\t,  UNK\t,  发展\t,  政策\t\n",
      "Nearest to 投资 : ,  方面\t,  风险\t,  预期\t,  股份\t,  市场\t,  建议\t,  行业\t,  发展\t\n",
      "Nearest to 影响 : ,  政策\t,  中国\t,  预期\t,  市场\t,  UNK\t,  板块\t,  风险\t,  股份\t\n",
      "Nearest to 数据 : ,  中国\t,  公司\t,  建议\t,  发展\t,  UNK\t,  市场\t,  风险\t,  政策\t\n",
      "Nearest to 竞争 : ,  方面\t,  板块\t,  企业\t,  风险\t,  行业\t,  市场\t,  中国\t,  重点\t\n",
      "Nearest to 估值 : ,  中国\t,  建议\t,  方面\t,  企业\t,  公司\t,  投资\t,  发展\t,  风险\t\n",
      "Nearest to 环比 : ,  企业\t,  发展\t,  公司\t,  行业\t,  方面\t,  股份\t,  板块\t,  建议\t\n",
      "Nearest to 国际 : ,  板块\t,  企业\t,  政策\t,  中国\t,  UNK\t,  发展\t,  投资\t,  公司\t\n",
      "Nearest to 国家 : ,  发展\t,  预期\t,  UNK\t,  企业\t,  重点\t,  股份\t,  风险\t,  公司\t\n",
      "Nearest to 公司 : ,  建议\t,  预期\t,  方面\t,  投资\t,  行业\t,  发展\t,  公司\t,  板块\t\n",
      "Nearest to 销售 : ,  市场\t,  重点\t,  预期\t,  建议\t,  板块\t,  政策\t,  风险\t,  公司\t\n",
      "Nearest to 布局 : ,  预期\t,  市场\t,  建议\t,  企业\t,  风险\t,  重点\t,  中国\t,  板块\t\n",
      "Nearest to 行情 : ,  板块\t,  UNK\t,  预期\t,  市场\t,  政策\t,  发展\t,  行业\t,  风险\t\n",
      "0.0192517317592\n",
      "now get data %s 7000\n",
      "Nearest to UNK : ,  投资\t,  行业\t,  市场\t,  重点\t,  板块\t,  公司\t,  中国\t,  风险\t\n",
      "Nearest to 预期 : ,  公司\t,  发展\t,  方面\t,  建议\t,  企业\t,  投资\t,  板块\t,  市场\t\n",
      "Nearest to 规模 : ,  UNK\t,  重点\t,  中国\t,  板块\t,  企业\t,  风险\t,  市场\t,  方面\t\n",
      "Nearest to 建设 : ,  风险\t,  行业\t,  方面\t,  投资\t,  建议\t,  UNK\t,  发展\t,  政策\t\n",
      "Nearest to 投资 : ,  方面\t,  风险\t,  预期\t,  股份\t,  市场\t,  建议\t,  行业\t,  发展\t\n",
      "Nearest to 影响 : ,  政策\t,  中国\t,  预期\t,  市场\t,  UNK\t,  板块\t,  风险\t,  股份\t\n",
      "Nearest to 数据 : ,  中国\t,  公司\t,  建议\t,  发展\t,  UNK\t,  市场\t,  风险\t,  政策\t\n",
      "Nearest to 竞争 : ,  方面\t,  板块\t,  企业\t,  风险\t,  行业\t,  市场\t,  中国\t,  重点\t\n",
      "Nearest to 估值 : ,  中国\t,  建议\t,  方面\t,  企业\t,  公司\t,  投资\t,  发展\t,  风险\t\n",
      "Nearest to 环比 : ,  企业\t,  发展\t,  公司\t,  行业\t,  方面\t,  股份\t,  板块\t,  建议\t\n",
      "Nearest to 国际 : ,  板块\t,  企业\t,  政策\t,  中国\t,  UNK\t,  发展\t,  投资\t,  公司\t\n",
      "Nearest to 国家 : ,  发展\t,  预期\t,  UNK\t,  企业\t,  重点\t,  股份\t,  风险\t,  公司\t\n",
      "Nearest to 公司 : ,  建议\t,  预期\t,  方面\t,  投资\t,  行业\t,  发展\t,  公司\t,  板块\t\n",
      "Nearest to 销售 : ,  市场\t,  重点\t,  预期\t,  建议\t,  板块\t,  政策\t,  风险\t,  公司\t\n",
      "Nearest to 布局 : ,  预期\t,  市场\t,  建议\t,  企业\t,  风险\t,  重点\t,  中国\t,  板块\t\n",
      "Nearest to 行情 : ,  板块\t,  UNK\t,  预期\t,  市场\t,  政策\t,  发展\t,  行业\t,  风险\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0162901332632\n",
      "now get data %s 8000\n",
      "Nearest to UNK : ,  投资\t,  行业\t,  市场\t,  重点\t,  板块\t,  公司\t,  中国\t,  风险\t\n",
      "Nearest to 预期 : ,  公司\t,  发展\t,  方面\t,  建议\t,  企业\t,  投资\t,  板块\t,  市场\t\n",
      "Nearest to 规模 : ,  UNK\t,  重点\t,  中国\t,  板块\t,  企业\t,  风险\t,  市场\t,  方面\t\n",
      "Nearest to 建设 : ,  风险\t,  行业\t,  方面\t,  投资\t,  建议\t,  UNK\t,  发展\t,  政策\t\n",
      "Nearest to 投资 : ,  方面\t,  风险\t,  预期\t,  股份\t,  市场\t,  建议\t,  行业\t,  发展\t\n",
      "Nearest to 影响 : ,  政策\t,  中国\t,  预期\t,  市场\t,  UNK\t,  板块\t,  风险\t,  股份\t\n",
      "Nearest to 数据 : ,  中国\t,  公司\t,  建议\t,  发展\t,  UNK\t,  市场\t,  风险\t,  政策\t\n",
      "Nearest to 竞争 : ,  方面\t,  板块\t,  企业\t,  风险\t,  行业\t,  市场\t,  中国\t,  重点\t\n",
      "Nearest to 估值 : ,  中国\t,  建议\t,  方面\t,  企业\t,  公司\t,  投资\t,  发展\t,  风险\t\n",
      "Nearest to 环比 : ,  企业\t,  发展\t,  公司\t,  行业\t,  方面\t,  股份\t,  板块\t,  建议\t\n",
      "Nearest to 国际 : ,  板块\t,  企业\t,  政策\t,  中国\t,  UNK\t,  发展\t,  投资\t,  公司\t\n",
      "Nearest to 国家 : ,  发展\t,  预期\t,  UNK\t,  企业\t,  重点\t,  股份\t,  风险\t,  公司\t\n",
      "Nearest to 公司 : ,  建议\t,  预期\t,  方面\t,  投资\t,  行业\t,  发展\t,  公司\t,  板块\t\n",
      "Nearest to 销售 : ,  市场\t,  重点\t,  预期\t,  建议\t,  板块\t,  政策\t,  风险\t,  公司\t\n",
      "Nearest to 布局 : ,  预期\t,  市场\t,  建议\t,  企业\t,  风险\t,  重点\t,  中国\t,  板块\t\n",
      "Nearest to 行情 : ,  板块\t,  UNK\t,  预期\t,  市场\t,  政策\t,  发展\t,  行业\t,  风险\t\n",
      "0.0155680614307\n",
      "now get data %s 9000\n",
      "Nearest to UNK : ,  投资\t,  行业\t,  市场\t,  重点\t,  板块\t,  公司\t,  中国\t,  风险\t\n",
      "Nearest to 预期 : ,  公司\t,  发展\t,  方面\t,  建议\t,  企业\t,  投资\t,  板块\t,  市场\t\n",
      "Nearest to 规模 : ,  UNK\t,  重点\t,  中国\t,  板块\t,  企业\t,  风险\t,  市场\t,  方面\t\n",
      "Nearest to 建设 : ,  风险\t,  行业\t,  方面\t,  投资\t,  建议\t,  UNK\t,  发展\t,  政策\t\n",
      "Nearest to 投资 : ,  方面\t,  风险\t,  预期\t,  股份\t,  市场\t,  建议\t,  行业\t,  发展\t\n",
      "Nearest to 影响 : ,  政策\t,  中国\t,  预期\t,  市场\t,  UNK\t,  板块\t,  风险\t,  股份\t\n",
      "Nearest to 数据 : ,  中国\t,  公司\t,  建议\t,  发展\t,  UNK\t,  市场\t,  风险\t,  政策\t\n",
      "Nearest to 竞争 : ,  方面\t,  板块\t,  企业\t,  风险\t,  行业\t,  市场\t,  中国\t,  重点\t\n",
      "Nearest to 估值 : ,  中国\t,  建议\t,  方面\t,  企业\t,  公司\t,  投资\t,  发展\t,  风险\t\n",
      "Nearest to 环比 : ,  企业\t,  发展\t,  公司\t,  行业\t,  方面\t,  股份\t,  板块\t,  建议\t\n",
      "Nearest to 国际 : ,  板块\t,  企业\t,  政策\t,  中国\t,  UNK\t,  发展\t,  投资\t,  公司\t\n",
      "Nearest to 国家 : ,  发展\t,  预期\t,  UNK\t,  企业\t,  重点\t,  股份\t,  风险\t,  公司\t\n",
      "Nearest to 公司 : ,  建议\t,  预期\t,  方面\t,  投资\t,  行业\t,  发展\t,  公司\t,  板块\t\n",
      "Nearest to 销售 : ,  市场\t,  重点\t,  预期\t,  建议\t,  板块\t,  政策\t,  风险\t,  公司\t\n",
      "Nearest to 布局 : ,  预期\t,  市场\t,  建议\t,  企业\t,  风险\t,  重点\t,  中国\t,  板块\t\n",
      "Nearest to 行情 : ,  板块\t,  UNK\t,  预期\t,  市场\t,  政策\t,  发展\t,  行业\t,  风险\t\n",
      "0.0170249648151\n",
      "now get data %s 10000\n",
      "Nearest to UNK : ,  投资\t,  行业\t,  市场\t,  重点\t,  板块\t,  公司\t,  中国\t,  风险\t\n",
      "Nearest to 预期 : ,  公司\t,  发展\t,  方面\t,  建议\t,  企业\t,  投资\t,  板块\t,  市场\t\n",
      "Nearest to 规模 : ,  UNK\t,  重点\t,  中国\t,  板块\t,  企业\t,  风险\t,  市场\t,  方面\t\n",
      "Nearest to 建设 : ,  风险\t,  行业\t,  方面\t,  投资\t,  建议\t,  UNK\t,  发展\t,  政策\t\n",
      "Nearest to 投资 : ,  方面\t,  风险\t,  预期\t,  股份\t,  市场\t,  建议\t,  行业\t,  发展\t\n",
      "Nearest to 影响 : ,  政策\t,  中国\t,  预期\t,  市场\t,  UNK\t,  板块\t,  风险\t,  股份\t\n",
      "Nearest to 数据 : ,  中国\t,  公司\t,  建议\t,  发展\t,  UNK\t,  市场\t,  风险\t,  政策\t\n",
      "Nearest to 竞争 : ,  方面\t,  板块\t,  企业\t,  风险\t,  行业\t,  市场\t,  中国\t,  重点\t\n",
      "Nearest to 估值 : ,  中国\t,  建议\t,  方面\t,  企业\t,  公司\t,  投资\t,  发展\t,  风险\t\n",
      "Nearest to 环比 : ,  企业\t,  发展\t,  公司\t,  行业\t,  方面\t,  股份\t,  板块\t,  建议\t\n",
      "Nearest to 国际 : ,  板块\t,  企业\t,  政策\t,  中国\t,  UNK\t,  发展\t,  投资\t,  公司\t\n",
      "Nearest to 国家 : ,  发展\t,  预期\t,  UNK\t,  企业\t,  重点\t,  股份\t,  风险\t,  公司\t\n",
      "Nearest to 公司 : ,  建议\t,  预期\t,  方面\t,  投资\t,  行业\t,  发展\t,  公司\t,  板块\t\n",
      "Nearest to 销售 : ,  市场\t,  重点\t,  预期\t,  建议\t,  板块\t,  政策\t,  风险\t,  公司\t\n",
      "Nearest to 布局 : ,  预期\t,  市场\t,  建议\t,  企业\t,  风险\t,  重点\t,  中国\t,  板块\t\n",
      "Nearest to 行情 : ,  板块\t,  UNK\t,  预期\t,  市场\t,  政策\t,  发展\t,  行业\t,  风险\t\n"
     ]
    }
   ],
   "source": [
    "#dataEncode, count, dictionary, reverse_dictionary = getWords() \n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    calLossStep=1000\n",
    "    print(stepsNum)\n",
    "    for step in range(stepsNum):\n",
    "        #batch_data, batch_labels = generate_batch(batch_size=batch_size, num_skips=2, skip_window=1, data=dataEncode)\n",
    "        #batch_data, batch_labels = generate_batch(batch_size=batch_size, num_skips=2, skip_window=1, data=dataEncode)\n",
    "\n",
    "        #print(\"data len is : \", len(batch_data), len(batch_labels))\n",
    "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        #print discard\n",
    "        average_loss += l\n",
    "        if (step % calLossStep) == 0:\n",
    "            print average_loss / calLossStep\n",
    "            average_loss = 0\n",
    "        sim = similarity.eval()\n",
    "        if (step % 1000) == 0:\n",
    "            print \"now get data %s\" , step\n",
    "            for i in range(valid_size):\n",
    "                idx = valid_examples[i]\n",
    "                valid_word = reverse_dictionary[idx]\n",
    "                top_k = 8\n",
    "                nearest = (-sim[i, :]).argsort()[1 : top_k + 1]\n",
    "                log = 'Nearest to %s : ' % valid_word\n",
    "                for top in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[top]]\n",
    "                    log =  \"%s,  %s\\t\" % (log, close_word)\n",
    "                print log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
